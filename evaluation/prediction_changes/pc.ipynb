{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def load_images_from_directory(root_path: str):\n",
    "    \"\"\"\n",
    "    Load images from a directory with subfolders named after ImageNet labels.\n",
    "    Return a list of (image, label, filename) triples.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    # Iterate over each subfolder\n",
    "    for label in os.listdir(root_path):\n",
    "        label_path = os.path.join(root_path, label)\n",
    "        \n",
    "        # Check if it's indeed a folder\n",
    "        if os.path.isdir(label_path):\n",
    "            \n",
    "            # Iterate over each image in the subfolder\n",
    "            for image_file in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_file)\n",
    "                \n",
    "                # Check if it's an image file\n",
    "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = Image.open(image_path)\n",
    "                    dataset.append((img, label, image_file))  # Add image filename here\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "current_dir = \"/home/workstation/code/XAImethods/CAIN\"\n",
    "\n",
    "# Load ImageNet class index\n",
    "with open(f\"{current_dir}/imagenet/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "\n",
    "\n",
    "label_to_index_description = {v[0]: (k, v[1]) for k, v in imagenet_class_index.items()}\n",
    "\n",
    "#all\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.special import kl_div as scipy_kl_div\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def closest_match(description: str, possible_descriptions: list) -> str:\n",
    "    \"\"\"\n",
    "    Find the closest match for the given description in the list of possible descriptions.\n",
    "    \"\"\"\n",
    "    matches = get_close_matches(description, possible_descriptions, n=1, cutoff=0.5)\n",
    "    return matches[0] if matches else description\n",
    "\n",
    "def predict_scores_for_classes(model, img_path):\n",
    "    if not os.path.exists(img_path):\n",
    "        return None\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((384, 384))\n",
    "    img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "    logits = model(img_tensor).logits[0].cpu().detach().numpy()\n",
    "    return logits\n",
    "\n",
    "def get_top_n_classes(scores, n=5):\n",
    "    return set(np.argsort(scores)[-n:])\n",
    "\n",
    "def calculate_jaccard_similarity(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def calculate_dice_similarity(set1, set2):\n",
    "    return 2 * len(set1.intersection(set2)) / (len(set1) + len(set2))\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    similarity = cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0]\n",
    "    normalized_similarity = (similarity + 1) / 2\n",
    "    return normalized_similarity\n",
    "\n",
    "def calculate_euclidean_distance(vec1, vec2):\n",
    "    distance = pairwise_distances(vec1.reshape(1, -1), vec2.reshape(1, -1), metric='euclidean')[0][0]\n",
    "    normalized_distance = distance / np.sqrt(len(vec1))\n",
    "    return normalized_distance\n",
    "\n",
    "def calculate_kl_divergence(vec1, vec2):\n",
    "    prob1 = softmax(vec1)\n",
    "    prob2 = softmax(vec2)\n",
    "    \n",
    "    kl_div_value = scipy_kl_div(prob1, prob2).sum()\n",
    "    normalized_kl_div = kl_div_value / np.log(len(vec1))\n",
    "    return normalized_kl_div\n",
    "\n",
    "def calculate_weighted_jaccard_similarity(vec1, vec2):\n",
    "    min_sum = sum([min(a, b) for a, b in zip(vec1, vec2)])\n",
    "    max_sum = sum([max(a, b) for a, b in zip(vec1, vec2)])\n",
    "    return min_sum / max_sum if max_sum != 0 else 0\n",
    "\n",
    "def calculate_prediction_changes(original_scores, masked_scores, top_n_indices):\n",
    "    # Assuming top_n_indices contains only one index for top1\n",
    "    idx = top_n_indices[0]\n",
    "    \n",
    "    original_score = original_scores[idx]\n",
    "    masked_score = masked_scores[idx]\n",
    "    \n",
    "    change = max(0, original_score - masked_score)  # Set to 0 if negative\n",
    "    \n",
    "    # Calculate the percentage change\n",
    "    if original_score > 0:\n",
    "        percentage = change / original_score\n",
    "    elif original_score < 0:\n",
    "        percentage = change / abs(original_score)\n",
    "    else:\n",
    "        percentage = 0\n",
    "    \n",
    "    return change, percentage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarities(original_scores, masked_scores, top_n_values=[1, 5, 10, 50], metrics=[\"jaccard\", \"dice\", \"cosine\", \"euclidean\", \"kl\", \"weighted_jaccard\", \"prediction_change\"]):\n",
    "    results = {}\n",
    "    \n",
    "    for n in top_n_values:\n",
    "        top_n_original = get_top_n_classes(original_scores, n)\n",
    "        top_n_masked = get_top_n_classes(masked_scores, n)\n",
    "        top_n_original_indices = np.argsort(original_scores)[-n:]\n",
    "\n",
    "        for metric in metrics:\n",
    "            if metric == \"jaccard\":\n",
    "                jaccard_similarity = calculate_jaccard_similarity(top_n_original, top_n_masked)\n",
    "                results[f\"Jaccard_Top_{n}\"] = jaccard_similarity\n",
    "            \n",
    "            if metric == \"dice\":\n",
    "                dice_similarity = calculate_dice_similarity(top_n_original, top_n_masked)\n",
    "                results[f\"Dice_Top_{n}\"] = dice_similarity\n",
    "                \n",
    "            if metric == \"cosine\":\n",
    "                cosine_sim = calculate_cosine_similarity(original_scores, masked_scores)\n",
    "                results[f\"Cosine_Top_{n}\"] = cosine_sim\n",
    "\n",
    "            if metric == \"euclidean\":\n",
    "                euclidean_dist = calculate_euclidean_distance(original_scores, masked_scores)\n",
    "                results[f\"Euclidean_Top_{n}\"] = euclidean_dist\n",
    "\n",
    "            if metric == \"kl\":\n",
    "                kl_div = calculate_kl_divergence(original_scores, masked_scores)\n",
    "                results[f\"KL_Top_{n}\"] = kl_div\n",
    "\n",
    "            if metric == \"weighted_jaccard\":\n",
    "                for n in top_n_values:\n",
    "                    original_top_n_indices = np.argsort(original_scores)[-n:]\n",
    "                    original_top_n_values = original_scores[original_top_n_indices]\n",
    "                    \n",
    "                    masked_top_n_indices = np.argsort(masked_scores)[-n:]\n",
    "                    masked_top_n_values = masked_scores[masked_top_n_indices]\n",
    "                    \n",
    "                    weighted_jaccard_sim = calculate_weighted_jaccard_similarity(original_top_n_values, masked_top_n_values)\n",
    "                    results[f\"Weighted_Jaccard_Top_{n}\"] = weighted_jaccard_sim\n",
    "\n",
    "            if metric == \"prediction_change\":\n",
    "                prediction_change, prediction_change_percentage = calculate_prediction_changes(original_scores, masked_scores, top_n_original_indices)\n",
    "                results[\"Prediction_Change_Top1\"] = prediction_change\n",
    "                results[\"Prediction_Change_Percentage_Top1\"] = prediction_change_percentage\n",
    "\n",
    "                \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.82it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb 单元格 2\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         similarities \u001b[39m=\u001b[39m calculate_similarities(original_scores, masked_scores, metrics\u001b[39m=\u001b[39mselected_metrics)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         np\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(subfolder_path, \u001b[39m'\u001b[39m\u001b[39msimilarity_metrics.npy\u001b[39m\u001b[39m'\u001b[39m), similarities)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m original_precision \u001b[39m=\u001b[39m precision_score(true_labels, original_predictions, average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmicro\u001b[39;49m\u001b[39m'\u001b[39;49m, zero_division\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, labels\u001b[39m=\u001b[39;49m[x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(label_to_index_description))])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m original_recall \u001b[39m=\u001b[39m recall_score(true_labels, original_predictions, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m, zero_division\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m original_f1 \u001b[39m=\u001b[39m f1_score(true_labels, original_predictions, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1776\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[1;32m   1648\u001b[0m     y_true,\n\u001b[1;32m   1649\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1655\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1656\u001b[0m ):\n\u001b[1;32m   1657\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \n\u001b[1;32m   1659\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1776\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1777\u001b[0m         y_true,\n\u001b[1;32m   1778\u001b[0m         y_pred,\n\u001b[1;32m   1779\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1780\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1781\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1782\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1783\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1784\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1785\u001b[0m     )\n\u001b[1;32m   1786\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1563\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1562\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1563\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1367\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1364\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m   1365\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m   1368\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1369\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/xai/lib/python3.10/site-packages/sklearn/utils/multiclass.py:109\u001b[0m, in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(\u001b[39misinstance\u001b[39m(label, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m ys_labels)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMix of label input types (string and number)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39msorted\u001b[39m(ys_labels))\n",
      "\u001b[0;31mValueError\u001b[0m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from transformers import CvtForImageClassification\n",
    "import shutil\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CvtForImageClassification.from_pretrained('microsoft/cvt-13').to(device)\n",
    "model.eval()\n",
    "\n",
    "masking_root_folder = \"/home/workstation/code/XAImethods/CAIN/evaluation_results/imagenet/val_images10k_attack/defocus_blur/1/facebook/convnext-tiny-224/GradCAM\"\n",
    "\n",
    "# Metrics to calculate\n",
    "selected_metrics = [\"kl\", \"prediction_change\"]\n",
    "\n",
    "true_labels = []\n",
    "original_predictions = []\n",
    "masked_predictions = []\n",
    "\n",
    "possible_descriptions = list(label_to_index_description.keys())\n",
    "\n",
    "for subfolder in tqdm(os.listdir(masking_root_folder)[:100]):\n",
    "    subfolder_path = os.path.join(masking_root_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        original_image_path = os.path.join(subfolder_path, 'original.jpg')\n",
    "        masked_image_path = os.path.join(subfolder_path, 'masked_image.jpg')\n",
    "        \n",
    "        # Extract true label from subfolder name\n",
    "        true_label = subfolder.split('_')[-1]\n",
    "        true_labels.append(label_to_index_description[true_label][0])\n",
    "        \n",
    "        original_scores = predict_scores_for_classes(model, original_image_path)\n",
    "        masked_scores = predict_scores_for_classes(model, masked_image_path)\n",
    "\n",
    "        original_pred = np.argmax(original_scores)\n",
    "        masked_pred = np.argmax(masked_scores)\n",
    "        \n",
    "        # Convert description to closest matching label\n",
    "        original_desc = model.config.id2label[original_pred]\n",
    "        closest_original_desc = closest_match(original_desc, possible_descriptions)\n",
    "        if closest_original_desc in label_to_index_description:\n",
    "            original_predictions.append(label_to_index_description[closest_original_desc][0])\n",
    "        else:\n",
    "            original_predictions.append(-1)  # Placeholder value indicating an unmatched description\n",
    "        \n",
    "        masked_desc = model.config.id2label[masked_pred]\n",
    "        closest_masked_desc = closest_match(masked_desc, possible_descriptions)\n",
    "        if closest_masked_desc in label_to_index_description:\n",
    "            masked_predictions.append(label_to_index_description[closest_masked_desc][0])\n",
    "        else:\n",
    "            masked_predictions.append(-1)  # Placeholder value indicating an unmatched description\n",
    "        \n",
    "        similarities = calculate_similarities(original_scores, masked_scores, metrics=selected_metrics)\n",
    "        np.save(os.path.join(subfolder_path, 'similarity_metrics.npy'), similarities)\n",
    "\n",
    "original_precision = precision_score(true_labels, original_predictions, average='micro', zero_division=1, labels=[x for x in range(len(label_to_index_description))])\n",
    "original_recall = recall_score(true_labels, original_predictions, average='micro', zero_division=1)\n",
    "original_f1 = f1_score(true_labels, original_predictions, average='micro')\n",
    "\n",
    "masked_precision = precision_score(true_labels, masked_predictions, average='micro', zero_division=1, labels=[x for x in range(len(label_to_index_description))])\n",
    "masked_recall = recall_score(true_labels, masked_predictions, average='micro', zero_division=1)\n",
    "masked_f1 = f1_score(true_labels, masked_predictions, average='micro')\n",
    "\n",
    "print(f\"Original images - Precision: {original_precision}, Recall: {original_recall}, F1: {original_f1}\")\n",
    "print(f\"Masked images - Precision: {masked_precision}, Recall: {masked_recall}, F1: {masked_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb 单元格 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOriginal images - Precision: \u001b[39m\u001b[39m{\u001b[39;00moriginal_precision\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Recall: \u001b[39m\u001b[39m{\u001b[39;00moriginal_recall\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, F1: \u001b[39m\u001b[39m{\u001b[39;00moriginal_f1\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/workstation/code/XAImethods/CAIN/evaluation/prediction_changes/pc.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMasked images - Precision: \u001b[39m\u001b[39m{\u001b[39;00mmasked_precision\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Recall: \u001b[39m\u001b[39m{\u001b[39;00mmasked_recall\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, F1: \u001b[39m\u001b[39m{\u001b[39;00mmasked_f1\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_precision' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Original images - Precision: {original_precision:.5f}, Recall: {original_recall:.5f}, F1: {original_f1:.5f}\")\n",
    "print(f\"Masked images - Precision: {masked_precision:.5f}, Recall: {masked_recall:.5f}, F1: {masked_f1:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(original_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # 这是一个用于数据可视化的Python库\n",
    "\n",
    "# 初始化一个用于存储所有度量数据的字典\n",
    "all_metrics = {}\n",
    "\n",
    "# 指定保存.npy文件的根目录\n",
    "root_folder = masking_root_folder\n",
    "\n",
    "# 遍历根目录下的所有子目录\n",
    "for subfolder in os.listdir(root_folder):\n",
    "    subfolder_path = os.path.join(root_folder, subfolder)\n",
    "    \n",
    "    # 检查是否为目录\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        npy_file_path = os.path.join(subfolder_path, 'similarity_metrics.npy')\n",
    "        \n",
    "        # 检查.npy文件是否存在\n",
    "        if os.path.exists(npy_file_path):\n",
    "            # 加载.npy文件\n",
    "            metrics = np.load(npy_file_path, allow_pickle=True).item()\n",
    "            \n",
    "            # 将这些度量值添加到all_metrics字典中\n",
    "            for key, value in metrics.items():\n",
    "                if key not in all_metrics:\n",
    "                    all_metrics[key] = []\n",
    "                all_metrics[key].append(value)\n",
    "\n",
    "# 绘制直方图和进行统计分析\n",
    "for metric, values in all_metrics.items():\n",
    "    if 'Prediction_Change_Top_1' not in metric and 'Prediction_Change_Percentage_Top_1' not in metric:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # 使用Seaborn库绘制带有KDE的直方图\n",
    "        sns.histplot(values, bins=20, kde=True)\n",
    "        \n",
    "        plt.title(f'Distribution of {metric}')\n",
    "        plt.xlabel(metric)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # 显示图像\n",
    "        plt.show()\n",
    "        \n",
    "        # 统计分析\n",
    "        mean_value = np.mean(values)\n",
    "        std_dev = np.std(values)\n",
    "        median_value = np.median(values)\n",
    "        quartiles = np.percentile(values, [25, 75])\n",
    "        \n",
    "        print(f\"=== Statistical Summary for {metric} ===\")\n",
    "        print(f\"Mean: {mean_value}\")\n",
    "        print(f\"Standard Deviation: {std_dev}\")\n",
    "        print(f\"Median: {median_value}\")\n",
    "        print(f\"1st Quartile: {quartiles[0]}\")\n",
    "        print(f\"3rd Quartile: {quartiles[1]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing required libraries\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Helper function to read all similarity metrics saved as .npy files into a list of dictionaries\n",
    "# def read_saved_metrics(root_folder):\n",
    "#     all_similarities = []\n",
    "#     for subfolder in os.listdir(root_folder):\n",
    "#         subfolder_path = os.path.join(root_folder, subfolder)\n",
    "#         if os.path.isdir(subfolder_path):\n",
    "#             similarity_file_path = os.path.join(subfolder_path, 'similarity_metrics.npy')\n",
    "#             if os.path.exists(similarity_file_path):\n",
    "#                 similarities = np.load(similarity_file_path, allow_pickle=True).item()\n",
    "#                 all_similarities.append(similarities)\n",
    "#     return all_similarities\n",
    "\n",
    "# # Root folder where all the similarity metrics are saved\n",
    "# root_folder = \"/home/workstation/code/XAImethods/hf_cam_dev/results/masked\"  # Replace with your directory\n",
    "\n",
    "# # Read the saved metrics\n",
    "# all_similarities = read_saved_metrics(root_folder)\n",
    "\n",
    "# # Convert the list of dictionaries to a DataFrame for easier manipulation\n",
    "# df = pd.DataFrame(all_similarities)\n",
    "\n",
    "# # Visualization and Analysis\n",
    "# metrics_to_analyze = [\"Jaccard_Top_1\", \"Dice_Top_1\", \"Cosine_Top_1\", \"Euclidean_Top_1\", \"KL_Top_1\"]\n",
    "\n",
    "# # Histograms\n",
    "# for metric in metrics_to_analyze:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.histplot(df[metric], bins=20, kde=True)\n",
    "#     plt.title(f'Distribution of {metric}')\n",
    "#     plt.xlabel(metric)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "# # Boxplots/Violin plots\n",
    "# for metric in metrics_to_analyze:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x=df[metric])\n",
    "#     plt.title(f'Boxplot of {metric}')\n",
    "#     plt.xlabel(metric)\n",
    "#     plt.show()\n",
    "\n",
    "# # Heatmap for correlation\n",
    "# correlation_matrix = df[metrics_to_analyze].corr()\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(correlation_matrix, annot=True)\n",
    "# plt.title('Heatmap of Correlations Between Metrics')\n",
    "# plt.show()\n",
    "\n",
    "# # Statistical Analysis\n",
    "# for metric in metrics_to_analyze:\n",
    "#     mean = df[metric].mean()\n",
    "#     std_dev = df[metric].std()\n",
    "#     median = df[metric].median()\n",
    "#     q1 = df[metric].quantile(0.25)\n",
    "#     q3 = df[metric].quantile(0.75)\n",
    "    \n",
    "#     print(f\"{metric}: Mean = {mean}, Std Dev = {std_dev}, Median = {median}, Q1 = {q1}, Q3 = {q3}\")\n",
    "\n",
    "# # For further analysis like clustering or anomaly detection, you can proceed with df DataFrame\n",
    "# # df now contains all your similarity metrics and can be used for advanced statistical methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the necessary libraries to read and display .npy file contents\n",
    "# import numpy as np\n",
    "\n",
    "# def display_npy_file_content(npy_file_path):\n",
    "#     \"\"\"\n",
    "#     Display the content of a .npy file.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(npy_file_path):\n",
    "#         return f\"File {npy_file_path} doesn't exist.\"\n",
    "#     npy_content = np.load(npy_file_path, allow_pickle=True).item()\n",
    "#     return npy_content\n",
    "\n",
    "# # Sample usage\n",
    "# # Assuming the file path is \"/path/to/your/file.npy\"\n",
    "# file_path = \"/home/workstation/code/XAImethods/hf_cam_dev/results/masked/ILSVRC2012_val_00000171/original_scores.npy\"\n",
    "\n",
    "# # Display the content\n",
    "# content = display_npy_file_content(file_path)\n",
    "# content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
