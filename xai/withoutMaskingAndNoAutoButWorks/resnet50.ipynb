{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/miniconda3/envs/xai/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision.models import resnet50\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from codecarbon import track_emissions\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam import (\n",
    "    GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
    "    AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n",
    "    LayerCAM, FullGrad, GradCAMElementWise\n",
    ")\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from typing import List, Callable, Optional\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load images\n",
    "def load_images_from_directory(root_path: str):\n",
    "    dataset = []\n",
    "    for label in os.listdir(root_path):\n",
    "        label_path = os.path.join(root_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for image_file in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_file)\n",
    "                if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img = Image.open(image_path)\n",
    "                    dataset.append((img, label, image_file))\n",
    "    return dataset\n",
    "\n",
    "current_dir = \"/home/z/Music/code/CAIN\"\n",
    "detail_dir = \"/imagenet/val_images10k_attack/defocus_blur/1\"\n",
    "\n",
    "dataset_path = f\"{current_dir}{detail_dir}\"\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_images_from_directory(dataset_path)\n",
    "with open(f\"{current_dir}/imagenet/imagenet_class_index.json\", \"r\") as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU!\")\n",
    "\n",
    "# ResNet Model Wrapper\n",
    "class ResNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ResNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "label_to_index_description = {v[0]: (k, v[1]) for k, v in imagenet_class_index.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:06<05:41,  3.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb 单元格 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m end_idx \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m((batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m BATCH_SIZE, \u001b[39mlen\u001b[39m(dataset))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Initialize ResNet50\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m model \u001b[39m=\u001b[39m resnet50(pretrained\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m target_layer_gradcam \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayer4[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mconv3  \u001b[39m# Last convolutional layer of ResNet50\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/code/CAIN/xai/withoutMaskingAndNoAutoButWorks/resnet50.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m ])\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/xai/lib/python3.8/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model and target layer\n",
    "model = resnet50(pretrained=True).to(device)\n",
    "model_wrapper = ResNetWrapper(model).to(device)\n",
    "target_layer_gradcam = model.layer4[-1].conv3\n",
    "\n",
    "# Options: GradCAM, HiResCAM, GradCAMPlusPlus, XGradCAM, \n",
    "# EigenGradCAM, LayerCAM, GradCAMElementWise\n",
    "# #    GradCAM, HiResCAM, GradCAMPlusPlus, XGradCAM, LayerCAM, GradCAMElementWise\n",
    "CAM_ALGORITHM = GradCAM\n",
    "cam_algorithm_name = CAM_ALGORITHM.__name__\n",
    "# Helper functions\n",
    "\n",
    "def run_grad_cam_on_image(model: torch.nn.Module,\n",
    "                          target_layer: torch.nn.Module,\n",
    "                          targets_for_gradcam: List[Callable],\n",
    "                          input_tensor: torch.nn.Module,\n",
    "                          input_image: Image,\n",
    "                          reshape_transform: Optional[Callable] = None,\n",
    "                          method: Callable = CAM_ALGORITHM):\n",
    "    with method(model=model,\n",
    "                target_layers=[target_layer],\n",
    "                reshape_transform=reshape_transform) as cam:\n",
    "        repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
    "        batch_results = cam(input_tensor=repeated_tensor,\n",
    "                            targets=targets_for_gradcam)\n",
    "        results = []\n",
    "        grayscale_cams = []\n",
    "        for grayscale_cam in batch_results:\n",
    "            visualization = show_cam_on_image(np.float32(input_image) / 255,\n",
    "                                              grayscale_cam,\n",
    "                                              use_rgb=True)\n",
    "            visualization = cv2.resize(visualization,\n",
    "                                       (visualization.shape[1] // 2, visualization.shape[0] // 2))\n",
    "            results.append(visualization)\n",
    "            grayscale_cams.append(grayscale_cam)\n",
    "        return np.hstack(results), grayscale_cams\n",
    "\n",
    "def print_top_categories(model, img_tensor, top_k=5):\n",
    "    logits = model(img_tensor.unsqueeze(0))\n",
    "    indices = logits.cpu()[0, :].detach().numpy().argsort()[-top_k :][::-1]\n",
    "    for i in indices:\n",
    "        print(f\"Predicted class {i}: {imagenet_class_index[str(i)][1]}\")\n",
    "\n",
    "def get_top_k_targets(model, input_tensor, k=5):\n",
    "    logits = model(input_tensor.unsqueeze(0))\n",
    "    top_k_indices = logits[0].argsort(descending=True)[:k].cpu().numpy()\n",
    "    return [ClassifierOutputTarget(index) for index in top_k_indices]\n",
    "\n",
    "# Prepare for the main loop\n",
    "BATCH_SIZE = 100\n",
    "num_batches = len(dataset) // BATCH_SIZE + (1 if len(dataset) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "save_dir = f\"{current_dir}/results/{detail_dir}/resnet50/{cam_algorithm_name}\"\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "def ensure_rgb(img):\n",
    "    if img.mode != 'RGB':\n",
    "        return img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "for batch_num in tqdm(range(num_batches)):\n",
    "    start_idx = batch_num * BATCH_SIZE\n",
    "    end_idx = min((batch_num + 1) * BATCH_SIZE, len(dataset))\n",
    "\n",
    "    # Initialize ResNet50\n",
    "    model = resnet50(pretrained=True).to(device)\n",
    "    target_layer_gradcam = model.layer4[-1].conv3  # Last convolutional layer of ResNet50\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        img, label, filename = dataset[idx]\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            img = ensure_rgb(img)\n",
    "            img_tensor = transform(img).to(device)\n",
    "\n",
    "            # Map label to ImageNet index\n",
    "            index_description = label_to_index_description.get(label)\n",
    "            if index_description is None:\n",
    "                print(f\"Warning: Label '{label}' not found in the JSON file!\")\n",
    "                continue\n",
    "\n",
    "            index_str, description = index_description\n",
    "            index = int(index_str)\n",
    "            dynamic_targets_for_gradcam = [ClassifierOutputTarget(index)]\n",
    "\n",
    "            gradcam_result, grayscale_cams = run_grad_cam_on_image(\n",
    "                model=model,\n",
    "                target_layer=target_layer_gradcam,\n",
    "                targets_for_gradcam=dynamic_targets_for_gradcam,\n",
    "                input_tensor=img_tensor,\n",
    "                input_image=img,\n",
    "                reshape_transform=None  # No reshape required for ResNet50\n",
    "            )\n",
    "\n",
    "            logits = model(img_tensor.unsqueeze(0))\n",
    "            top_indices = logits[0].argsort(descending=True)[:5].cpu().numpy()\n",
    "            predictions = {index: {\"score\": logits[0][index].item(), \"label\": imagenet_class_index[str(index)][1]} for index in top_indices}\n",
    "\n",
    "            img_dir = os.path.join(save_dir, filename.rsplit('.', 1)[0])\n",
    "            if not os.path.exists(img_dir):\n",
    "                os.makedirs(img_dir)\n",
    "\n",
    "            true_label_file = os.path.join(img_dir, 'true_label.txt')\n",
    "            with open(true_label_file, 'w') as f:\n",
    "                f.write(str(label))\n",
    "\n",
    "            img_name = os.path.join(img_dir, \"original.jpg\")\n",
    "            gradcam_name = os.path.join(img_dir, \"gradcam.jpg\")\n",
    "            grayscale_name = os.path.join(img_dir, \"grayscale.jpg\")\n",
    "            grayscale_npy_name = os.path.join(img_dir, \"grayscale.npy\")\n",
    "            scores_name = os.path.join(img_dir, \"scores.npy\")\n",
    "            info_name = os.path.join(img_dir, \"info.txt\")\n",
    "\n",
    "            img.save(img_name)\n",
    "            Image.fromarray(gradcam_result).save(gradcam_name)\n",
    "            Image.fromarray((grayscale_cams[0] * 255).astype(np.uint8)).save(grayscale_name)\n",
    "            np.save(grayscale_npy_name, grayscale_cams[0])\n",
    "\n",
    "            scores = [data[\"score\"] for _, data in predictions.items()]\n",
    "            np.save(scores_name, scores)\n",
    "\n",
    "            with open(info_name, 'w') as f:\n",
    "                for index, data in predictions.items():\n",
    "                    label = data[\"label\"]\n",
    "                    score = data[\"score\"]\n",
    "                    f.write(f\"Class {index} ({label}): {score:.2f}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "print(\"Grad-CAM processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [02:54<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad-CAM processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
